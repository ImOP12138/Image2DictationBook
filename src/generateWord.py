import requests
from docx import Document
from docx.enum.text import WD_UNDERLINE, WD_ALIGN_PARAGRAPH
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.shared import Pt, Cm
import os
import re
import json # éœ€è¦å¯¼å…¥ json

from LLMAPI import call_doubao_model # å‡è®¾ generateWord.py ä¹Ÿåœ¨ src ç›®å½•ä¸‹
# APIé…ç½®
API_URL = "https://v2.xxapi.cn/api/englishwords"
HEADERS = {
    'User-Agent': 'xiaoxiaoapi/1.0.0 (https://xxapi.cn)'
}
# --- è±†åŒ…æ¨¡å‹é…ç½® ---
DOUBAO_MODEL_NAME = "doubao-seed-1-6-flash-250615" # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹ID

# --- è±†åŒ…æ¨¡å‹è°ƒç”¨ ---
def call_large_model_api(words_batch):
    """
    è°ƒç”¨è±†åŒ…å¤§æ¨¡å‹ API æ¥æ‰¹é‡ç¿»è¯‘å•è¯ã€‚
    """
    print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨è±†åŒ…æ¨¡å‹ç¿»è¯‘ {len(words_batch)} ä¸ªå•è¯...")

    # --- æ„é€ æç¤ºè¯ ---
    prompt = (
        "è¯·ä¸ºä»¥ä¸‹è‹±æ–‡å•è¯æˆ–çŸ­è¯­æä¾›ä¸­æ–‡é‡Šä¹‰ã€‚"
        "è¦æ±‚ï¼š1. åªè¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œç»“æ„ä¸º {\"translations\": [{\"word\": \"...\", \"meaning\": \"...\\n...\"}, ...]}ã€‚"
        "2. 'meaning' å­—æ®µå†…ï¼Œæ¯ä¸ªé‡Šä¹‰é¡¹ç”¨ '\\n' åˆ†éš”ï¼Œæ ¼å¼ä¸º 'è¯æ€§ï¼ˆç”¨nã€adjã€vç­‰è¿™ç§å¸¸ç”¨è‹±æ–‡å­—æ¯è¡¨ç¤ºçš„æ–¹å¼è¡¨è¾¾ï¼‰. é‡Šä¹‰'ã€‚"
        "3. ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šã€è¯´æ˜æˆ– Markdownã€‚"
        "4. ä¸¥æ ¼æŒ‰ç…§æä¾›çš„å•è¯åˆ—è¡¨é¡ºåºè¿”å›ã€‚"
        "å•è¯åˆ—è¡¨: "
        + ", ".join([f'"{word}"' for word in words_batch])
    )
    print(f"ğŸ“ å‘é€ç»™å¤§æ¨¡å‹çš„æç¤ºè¯: {prompt}") # ä»…ç”¨äºè°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒå¯ç§»é™¤

    # --- è°ƒç”¨å°è£…å¥½çš„å‡½æ•° ---
    raw_response_text = call_doubao_model(DOUBAO_MODEL_NAME, prompt)

    if raw_response_text is None:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œcall_doubao_model å·²ç»æ‰“å°äº†é”™è¯¯æ—¥å¿—
        # è¿™é‡Œå¯ä»¥è¿”å›ä¸€ä¸ªè¡¨ç¤ºé”™è¯¯çš„ç»“æ„
        return {"translations": []}

    # --- å°è¯•è§£æè¿”å›çš„ JSON ---
    try:
        # å¤§æ¨¡å‹æœ‰æ—¶ä¼šåœ¨ JSON å‰ååŠ ä¸Š ```json ``` æ ‡è®°ï¼Œéœ€è¦ç§»é™¤
        cleaned_text = raw_response_text.strip()
        if cleaned_text.startswith("```json"):
            cleaned_text = cleaned_text[7:] # ç§»é™¤ ```json
        if cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-3] # ç§»é™¤ ```

        model_data = json.loads(cleaned_text)
        print(f"ğŸ¤– å¤§æ¨¡å‹æˆåŠŸè§£æ JSON: {json.dumps(model_data, indent=2, ensure_ascii=False)}") # ä»…ç”¨äºè°ƒè¯•
        return model_data
    except json.JSONDecodeError as e:
        print(f"âŒ æ— æ³•å°†å¤§æ¨¡å‹çš„å›å¤è§£æä¸º JSON: {e}")
        print(f"ğŸ¤– å¤§æ¨¡å‹çš„åŸå§‹å›å¤æ˜¯: {raw_response_text}")
        # è¿”å›ä¸€ä¸ªç©ºçš„æˆ–é”™è¯¯çš„ç»“æ„
        return {"translations": []}
# --- æ›¿æ¢æˆ–ä¿®æ”¹ç»“æŸ ---





def get_word_details(word):
    """è°ƒç”¨å°å°APIè·å–å•è¯è¯¦ç»†ä¿¡æ¯"""
    try:
        # æ³¨æ„ï¼šåŸä»£ç  URL å’Œ Headers æœ«å°¾æœ‰ç©ºæ ¼ï¼Œå·²ä¿®æ­£
        response = requests.get(f"{API_URL}?word={word}", headers=HEADERS)
        response.raise_for_status() # æ›´å¥½çš„é”™è¯¯å¤„ç†
        data = response.json()

        if data.get("code") == 200: # ä½¿ç”¨ .get() æ›´å®‰å…¨
            translations = data["data"]["translations"]
            result = []

            for trans in translations:
                pos = trans["pos"]
                tran_cn = trans["tran_cn"].strip()
                result.append(f"{pos}. {tran_cn}")

            return "\n".join(result) # è¿”å›æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²

        else:
            print(f"âš ï¸ å°å°APIæœªæ‰¾åˆ° '{word}' çš„é‡Šä¹‰ (Code: {data.get('code')})")
            return "æœªæ‰¾åˆ°é‡Šä¹‰"
    except requests.exceptions.RequestException as e: # æ›´å…·ä½“çš„å¼‚å¸¸å¤„ç†
        print(f"âŒ å°å°APIè¯·æ±‚å¤±è´¥ '{word}': {e}")
        return f"è¯·æ±‚å¤±è´¥ï¼š{e}"
    except (KeyError, json.JSONDecodeError) as e: # å¤„ç† JSON è§£ææˆ–é”®ä¸å­˜åœ¨é”™è¯¯
        print(f"âŒ å°å°APIå“åº”æ ¼å¼é”™è¯¯ '{word}': {e}")
        return "è¯·æ±‚å¤±è´¥ï¼šå“åº”æ ¼å¼é”™è¯¯"

# --- ä¿®æ”¹ï¼šgenerate_dictation_books ä¸»å‡½æ•° ---
def generate_dictation_books(input_file='word.txt'):
    """ç”Ÿæˆå¬å†™æœ¬çš„ä¸»å‡½æ•°"""
    try:
        words = read_words_from_file(input_file)
        if not words:
            return False, "æ²¡æœ‰æ‰¾åˆ°å•è¯æ•°æ®"

        words_with_details = []
        words_for_llm = [] # å­˜å‚¨éœ€è¦å¤§æ¨¡å‹ç¿»è¯‘çš„å•è¯

        # ç¬¬ä¸€æ­¥ï¼šè°ƒç”¨å°å°APIè·å–é‡Šä¹‰
        for word in words:
            meaning = get_word_details(word)
            if meaning == "æœªæ‰¾åˆ°é‡Šä¹‰":
                words_for_llm.append(word)
                # å…ˆå­˜ä¸ªå ä½ç¬¦ï¼Œåç»­ç”¨å¤§æ¨¡å‹ç»“æœæ›¿æ¢
                words_with_details.append((word, "å¾…å¤§æ¨¡å‹ç¿»è¯‘..."))
            else:
                words_with_details.append((word, meaning))

        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è°ƒç”¨å¤§æ¨¡å‹å¤„ç†å¤±è´¥çš„å•è¯
        BATCH_SIZE = 20
        llm_results_dict = {} # ç”¨å­—å…¸å­˜å‚¨ç»“æœï¼Œæ–¹ä¾¿åç»­æŸ¥æ‰¾ {word: meaning}

        for i in range(0, len(words_for_llm), BATCH_SIZE):
            batch = words_for_llm[i:i + BATCH_SIZE]
            try:
                llm_response_data = call_large_model_api(batch)
                # è§£æå¤§æ¨¡å‹è¿”å›çš„ JSON
                if "translations" in llm_response_data:
                    for item in llm_response_data["translations"]:
                        llm_results_dict[item["word"]] = item["meaning"]
                else:
                     print(f"âš ï¸ å¤§æ¨¡å‹è¿”å›æ•°æ®æ ¼å¼ä¸æ­£ç¡® (ç¼ºå°‘ 'translations' é”®): {llm_response_data}")
            except Exception as e:
                 print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹æˆ–è§£æå…¶å“åº”æ—¶å‡ºé”™: {e}")
                 # å¯ä»¥é€‰æ‹©ä¸ºè¿™æ‰¹å•è¯è®¾ç½®ä¸€ä¸ªé»˜è®¤é”™è¯¯ä¿¡æ¯
                 for word in batch:
                     llm_results_dict[word] = f"å¤§æ¨¡å‹ç¿»è¯‘å¤±è´¥: {e}"

        # ç¬¬ä¸‰æ­¥ï¼šå°†å¤§æ¨¡å‹çš„ç»“æœæ•´åˆå› words_with_details
        for i, (word, meaning) in enumerate(words_with_details):
             if meaning == "å¾…å¤§æ¨¡å‹ç¿»è¯‘...":
                 # ç”¨å¤§æ¨¡å‹çš„ç»“æœæ›¿æ¢å ä½ç¬¦
                 words_with_details[i] = (word, llm_results_dict.get(word, "å¤§æ¨¡å‹æœªè¿”å›é‡Šä¹‰"))

        # ç¬¬å››æ­¥ï¼šç”ŸæˆWordæ–‡æ¡£
        success1 = create_word_doc(words_with_details, 'å•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx')
        success2 = create_blank_word_doc(words_with_details, 'å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx')

        if success1 and success2:
            return True, "å¬å†™æœ¬ç”ŸæˆæˆåŠŸï¼å·²åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼šå•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx å’Œ å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx"
        else:
            return False, "éƒ¨åˆ†æ–‡ä»¶ç”Ÿæˆå¤±è´¥"

    except Exception as e:
        import traceback
        traceback.print_exc() # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆ
        return False, f"ç”Ÿæˆå¬å†™æœ¬å¤±è´¥ï¼š{str(e)}"

# --- å…¶ä»–å‡½æ•° (read_words_from_file, create_word_doc, create_blank_word_doc) ä¿æŒä¸å˜ ---
# (ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œä¹ŸåŒ…å«å®ƒä»¬ï¼Œä½†å®é™…ä½¿ç”¨æ—¶ä¸éœ€è¦é‡å¤)
def read_words_from_file(filename='word.txt'):
    """è¯»å–txtä¸­çš„å•è¯"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            words = [line.strip() for line in f.readlines() if line.strip()]
        return words
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return []

def create_word_doc(words_with_details, output_filename='å•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx'):
    """åˆ›å»ºå¸¦è¯æ„çš„å¬å†™æœ¬word"""
    try:
        doc = Document()
        section = doc.sections[0]
        sect_pr = section._sectPr
        cols = OxmlElement('w:cols')
        cols.set(qn('w:num'), '2')
        cols.set(qn('w:space'), '720')
        for child in list(sect_pr):
            if child.tag == qn('w:cols'):
                sect_pr.remove(child)
        sect_pr.append(cols)
        table = doc.add_table(rows=0, cols=2)
        for word, detail in words_with_details:
            row = table.add_row()
            cells = row.cells
            cells[0].text = word
            cells[1].text = ''
            paragraph = cells[1].paragraphs[0]
            paragraph.paragraph_format.space_after = Pt(0)
            lines = detail.split('\n')
            for i, line in enumerate(lines):
                run = paragraph.add_run(line.strip())
                run.bold = False
                run.font.name = 'å®‹ä½“'
                run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
                run.font.size = Pt(9)
                if i < len(lines) - 1:
                    run.add_break()
            run.underline = WD_UNDERLINE.SINGLE
            cells[0].width = 12800
        doc.save(output_filename)
        print(f"âœ… å·²ä¿å­˜ä¸º {os.path.abspath(output_filename)}")
        return True
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¸¦è¯æ„æ–‡æ¡£å¤±è´¥ï¼š{e}")
        return False

def create_blank_word_doc(words_with_details, output_filename='å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx'):
    """åˆ›å»ºä¸å¸¦è¯æ„çš„å¬å†™æœ¬wordï¼ˆä¾›é»˜å†™ï¼‰"""
    try:
        doc = Document()
        section = doc.sections[0]
        sect_pr = section._sectPr
        cols = OxmlElement('w:cols')
        cols.set(qn('w:num'), '2')
        cols.set(qn('w:space'), '720')
        for child in list(sect_pr):
            if child.tag == qn('w:cols'):
                sect_pr.remove(child)
        sect_pr.append(cols)
        table = doc.add_table(rows=0, cols=2)
        for word, detail in words_with_details:
            row = table.add_row()
            cells = row.cells
            cells[0].text = word
            cells[1].text = ''
            paragraph = cells[1].paragraphs[0]
            paragraph.paragraph_format.space_after = Pt(0)
            lines = detail.split('\n')
            for i, line in enumerate(lines):
                match = re.match(r'^([a-zA-Z]+\.).*', line.strip())
                if match:
                    pos = match.group(1)
                    run = paragraph.add_run(pos)
                    run.font.name = 'å®‹ä½“'
                    run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
                    run.font.size = Pt(9)
                    if i < len(lines) - 1:
                        run.add_break()
            cells[0].width = 12800
        doc.save(output_filename)
        print(f"âœ… å·²ä¿å­˜ä¸º {os.path.abspath(output_filename)}")
        return True
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ— è¯æ„æ–‡æ¡£å¤±è´¥ï¼š{e}")
        return False

# # --- ä¸»ç¨‹åºå…¥å£ ---
# if __name__ == "__main__":
#     success, message = generate_dictation_books('word.txt')
#     print(message)
























# # src/generateWord.py
# import requests
# from docx import Document
# from docx.enum.text import WD_UNDERLINE, WD_ALIGN_PARAGRAPH
# from docx.oxml import OxmlElement
# from docx.oxml.ns import qn
# from docx.shared import Pt, Cm
# import os
# import re
#
# # APIé…ç½®
# API_URL = "https://v2.xxapi.cn/api/englishwords"
# HEADERS = {
#     'User-Agent': 'xiaoxiaoapi/1.0.0 (https://xxapi.cn)'
# }
#
#
# def read_words_from_file(filename='word.txt'):
#     """è¯»å–txtä¸­çš„å•è¯"""
#     try:
#         with open(filename, 'r', encoding='utf-8') as f:
#             words = [line.strip() for line in f.readlines() if line.strip()]
#         return words
#     except FileNotFoundError:
#         print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
#         return []
#     except Exception as e:
#         print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
#         return []
#
#
# def get_word_details(word):
#     """è°ƒç”¨å°å°APIè·å–å•è¯è¯¦ç»†ä¿¡æ¯"""
#     try:
#         response = requests.get(f"{API_URL}?word={word}", headers=HEADERS)
#         data = response.json()
#
#         if data["code"] == 200:
#             translations = data["data"]["translations"]
#             result = []
#
#             for trans in translations:
#                 pos = trans["pos"]
#                 tran_cn = trans["tran_cn"].strip()
#                 result.append(f"{pos}. {tran_cn}")
#
#             # è¿”å›æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
#             return "\n".join(result)
#
#         else:
#             return "æœªæ‰¾åˆ°é‡Šä¹‰"
#     except Exception as e:
#         return f"è¯·æ±‚å¤±è´¥ï¼š{e}"
#
#
# def create_word_doc(words_with_details, output_filename='å•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx'):
#     """åˆ›å»ºå¸¦è¯æ„çš„å¬å†™æœ¬word"""
#     try:
#         doc = Document()
#
#         # è®¾ç½®åŒæ å¸ƒå±€
#         section = doc.sections[0]
#         sect_pr = section._sectPr
#
#         cols = OxmlElement('w:cols')
#         cols.set(qn('w:num'), '2')  # è®¾ç½®ä¸¤æ 
#         cols.set(qn('w:space'), '720')  # æ é—´è·ï¼ˆå•ä½ twipï¼‰
#
#         # æ¸…é™¤æ—§çš„åˆ†æ è®¾ç½®
#         for child in list(sect_pr):
#             if child.tag == qn('w:cols'):
#                 sect_pr.remove(child)
#
#         sect_pr.append(cols)
#
#         # æ·»åŠ è¡¨æ ¼
#         table = doc.add_table(rows=0, cols=2)
#
#         # è®¾ç½®ç¬¬ä¸€åˆ—å®½åº¦
#         for row in table.rows:
#             row.cells[0].width = 12800  # å•ä½ï¼šç¼‡ï¼ˆtwipï¼‰
#
#         # å¡«å……æ•°æ®
#         for word, detail in words_with_details:
#             row = table.add_row()
#             cells = row.cells  # è·å–å½“å‰è¡Œçš„å•å…ƒæ ¼
#             cells[0].text = word
#             cells[1].text = ''
#
#             paragraph = cells[1].paragraphs[0]
#             paragraph.paragraph_format.space_after = Pt(0)  # è¡Œè·ç´§å‡‘
#             lines = detail.split('\n')
#             for i, line in enumerate(lines):
#                 run = paragraph.add_run(line.strip())
#                 run.bold = False  # å¼ºåˆ¶ä¸åŠ ç²—
#                 run.font.name = 'å®‹ä½“'  # è®¾ç½®ä¸­æ–‡å­—ä½“
#                 run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')  # å…¼å®¹ä¸­æ–‡å­—ä½“
#                 run.font.size = Pt(9)  # è®¾ç½®å­—å·
#
#                 if i < len(lines) - 1:
#                     run.add_break()  # æ¢è¡Œ
#
#             # æœ€åä¸€è¡Œä¹Ÿç»Ÿä¸€è®¾ç½®
#             run.underline = WD_UNDERLINE.SINGLE
#
#             # è®¾ç½®æ–°å¢è¡Œçš„ç¬¬ä¸€åˆ—å®½åº¦
#             cells[0].width = 12800
#
#         doc.save(output_filename)
#         print(f"âœ… å·²ä¿å­˜ä¸º {os.path.abspath(output_filename)}")
#         return True
#
#     except Exception as e:
#         print(f"âŒ ç”Ÿæˆå¸¦è¯æ„æ–‡æ¡£å¤±è´¥ï¼š{e}")
#         return False
#
#
# def create_blank_word_doc(words_with_details, output_filename='å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx'):
#     """åˆ›å»ºä¸å¸¦è¯æ„çš„å¬å†™æœ¬wordï¼ˆä¾›é»˜å†™ï¼‰"""
#     try:
#         doc = Document()
#
#         # è®¾ç½®åŒæ å¸ƒå±€
#         section = doc.sections[0]
#         sect_pr = section._sectPr
#
#         cols = OxmlElement('w:cols')
#         cols.set(qn('w:num'), '2')  # è®¾ç½®ä¸¤æ 
#         cols.set(qn('w:space'), '720')  # æ é—´è·ï¼ˆå•ä½ twipï¼‰
#
#         # æ¸…é™¤æ—§çš„åˆ†æ è®¾ç½®
#         for child in list(sect_pr):
#             if child.tag == qn('w:cols'):
#                 sect_pr.remove(child)
#
#         sect_pr.append(cols)
#
#         # æ·»åŠ è¡¨æ ¼ï¼ˆä¸å¸¦åˆå§‹è¡Œï¼‰
#         table = doc.add_table(rows=0, cols=2)
#
#         # å¡«å……æ•°æ®
#         for word, detail in words_with_details:
#             row = table.add_row()
#             cells = row.cells
#             cells[0].text = word
#             cells[1].text = ''
#
#             paragraph = cells[1].paragraphs[0]
#             paragraph.paragraph_format.space_after = Pt(0)
#
#             # æå–è¯æ€§éƒ¨åˆ†ï¼ˆn., v., adj. ç­‰ï¼‰
#             lines = detail.split('\n')
#             for i, line in enumerate(lines):
#                 match = re.match(r'^([a-zA-Z]+\.).*', line.strip())
#                 if match:
#                     pos = match.group(1)
#                     run = paragraph.add_run(pos)
#                     run.font.name = 'å®‹ä½“'
#                     run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
#                     run.font.size = Pt(9)
#                     if i < len(lines) - 1:
#                         run.add_break()  # æ¯ä¸ªè¯æ€§å ä¸€è¡Œ
#
#             # è®¾ç½®ç¬¬ä¸€åˆ—å®½åº¦
#             cells[0].width = 12800
#
#         doc.save(output_filename)
#         print(f"âœ… å·²ä¿å­˜ä¸º {os.path.abspath(output_filename)}")
#         return True
#
#     except Exception as e:
#         print(f"âŒ ç”Ÿæˆæ— è¯æ„æ–‡æ¡£å¤±è´¥ï¼š{e}")
#         return False
#
#
# def generate_dictation_books(input_file='word.txt'):
#     """ç”Ÿæˆå¬å†™æœ¬çš„ä¸»å‡½æ•°"""
#     try:
#         # è¯»å–å•è¯
#         words = read_words_from_file(input_file)
#         if not words:
#             return False, "æ²¡æœ‰æ‰¾åˆ°å•è¯æ•°æ®"
#
#         # è·å–æ¯ä¸ªå•è¯çš„è¯¦ç»†ä¿¡æ¯
#         words_with_details = []
#         for word in words:
#             meaning = get_word_details(word)
#             words_with_details.append((word, meaning))
#
#         # ç”ŸæˆWordæ–‡æ¡£
#         success1 = create_word_doc(words_with_details, 'å•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx')
#
#         # ç”Ÿæˆä¸å¸¦è¯æ„çš„ Word æ–‡æ¡£ï¼ˆä¾›é»˜å†™ï¼‰
#         success2 = create_blank_word_doc(words_with_details, 'å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx')
#
#         if success1 and success2:
#             return True, "å¬å†™æœ¬ç”ŸæˆæˆåŠŸï¼å·²åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼šå•è¯å¬å†™æœ¬ï¼ˆå¸¦è¯æ„ï¼‰.docx å’Œ å•è¯å¬å†™æœ¬ï¼ˆæ— è¯æ„ï¼‰.docx"
#         else:
#             return False, "éƒ¨åˆ†æ–‡ä»¶ç”Ÿæˆå¤±è´¥"
#
#     except Exception as e:
#         return False, f"ç”Ÿæˆå¬å†™æœ¬å¤±è´¥ï¼š{str(e)}"